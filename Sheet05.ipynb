{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OsnabrÃ¼ck University - A&C: Computational Cognition (Summer Term 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 05: Eye tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in at 14:00 at **Tuesday, May 28, 2019**. If you need help (and Google and other resources were not enough), feel free to contact your tutors. Please push your results to your Github group folder.\n",
    "\n",
    "For this exercise sheet you will have 2 weeks and the sheet is also worth of 30 points. In this exercise sheet you will start to work with eye tracking data. Note that the data we will use here are not raw gaze data and contain coordinates of fixation points.\n",
    "\n",
    "The dataset is distributed freely by a following study: [Wilming, N. Dryad](https://www.nature.com/articles/sdata2016126#data-citations). By clicking on the link in the section \"Data Citations\" you will get redirected to the page where you can download all the data openly distributed. Read below for description of each file.\n",
    "\n",
    "##### necessary\n",
    "*etdb_v1.0*: This is the main data file. The hdf5 file consists of all the fixation data and the metadata.  \n",
    "*Read gaze data with python*: Python script to read hdf5 file as a dataframe.  \n",
    "*Stimuli/i*: Zip file containing image stimuli used in the study. The encoding convention is same as in the dataframe.\n",
    "##### optional\n",
    "*Metadata*: This is the csv file giving overview of all studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install h5py # make sure to install h5py which is used in fixmat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.anova import *\n",
    "from statsmodels.formula.api import ols\n",
    "from datas.fixmat import *\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 0: Peer review for sheet 04 [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group reviews the solutions of two other groups and give points according to the given point distribution considering the correctness of the solution. For this reviews the tutors will give you up to 3 points each week. Follow a distributed comment guidelines if you are unsure.\n",
    "\n",
    "| * |Group 1|Group 2|Group 3|Group 4|Group 5|Group 6|Group 7|Group 8|Group 9|Group 10|Group 11|\n",
    "| ------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ------ | ------ |\n",
    "| check solutions of group: | 5, 9 | 1, 6  | 4, 7  | 7, 2 | 2, 11 | 8, 3 | 3, 10  | 11, 1  | 10, 4  | 6, 8  | 9, 5   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Checking the data distribution [9 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) loading datasets [0 pts]\n",
    "As with any other datasets, the first step is to get an idea of the dataset. Check the meta data and column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a fixmat.py to load the hdf5 format data as a panda dataframe\n",
    "df, meta = load('datas/etdb_v1.0.hdf5', \"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) data cleaning [3 pts]\n",
    "We want to clean the dataframe so that it is handy for us to work with it.\n",
    "- How many fixations do we have per category? ```agg``` function might be helpful.\n",
    "- categories are encoded using a number. Add a column to the dataframe that has approporiate string value for that category (e.g. code 11.0 to \"Pink-noise\"). The category names can be found on Figure 2 of the paper.\n",
    "- since measurements lasted for 6 seconds, any fixation points that have a onset time before 0 sec and end time more than 6 sec are erroneous. Remove these rows.\n",
    "- also remove all rows with any NaN values.\n",
    "- add a column called ```duration``` and compute the duration of each fixation.\n",
    "- It is known from previous literature that fixations typically last between 100 msec to 400 msec. Remove all rows with unrealistic  fixation duration.\n",
    "- check how many data points got removed for each category. Let's hope that we didn't delete too many rows from a single category.\n",
    "- count the number of fixations for each trial. To do this, you can use the aggregate method to count the number of rows for each category.\n",
    "- print the mean duration and the mean number of fixation across all trials. Are they in a realistic range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add column with decoded categories\n",
    "df['decoded category'] = df['category'].map({7: 'Natural', 8: 'Urban', 10: 'Fractals', 11: 'Pink Noise'})\n",
    "\n",
    "# Number of fixations per category\n",
    "fix_points_cat = df.groupby(['decoded category']).count()['SUBJECTINDEX']\n",
    "# Remove all fixations befor the stimulus onset\n",
    "clean_df = df[df['start'] >= 0]\n",
    "# Remove all fixations after stimulus offsetand remove all lines with NaN values.\n",
    "clean_df = clean_df[clean_df['end'] <= 6000].dropna()\n",
    "# Add duration column\n",
    "clean_df['duration'] = clean_df['end'] - clean_df['start']\n",
    "# Clean all lines with unrealistic duration\n",
    "clean_df = clean_df[(clean_df['duration'] <= 400) & (clean_df['duration'] >= 100)]\n",
    "# Count the number of removed points per category\n",
    "removed_points = (fix_points_cat - clean_df.groupby(['decoded category']).count()['SUBJECTINDEX'])\n",
    "# Group dataframe be subject index and trial. Count for every trial the number of fixations\n",
    "fixations_per_trial = clean_df.groupby(['SUBJECTINDEX','trial'])['category'].count()\n",
    "# Calculate the mean number of fixations\n",
    "mean_fixations = fixations_per_trial.mean()\n",
    "# Calculate the mean duration of fixations\n",
    "mean_duration = clean_df['duration'].mean()\n",
    "# Print results\n",
    "\n",
    "print('Number of removed points per category: \\n', removed_points)\n",
    "print()\n",
    "print('Number of remaining points in category: \\n', clean_df.groupby(['decoded category']).count()['SUBJECTINDEX'])\n",
    "print()\n",
    "print('Mean number of fixations per trial: ', mean_fixations)\n",
    "print('Mean duration of fixations: ', mean_duration)\n",
    "print('Time covered per trial in the mean: 12.5*240 ms =', 12.5* 2400, 'ms')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining dataset is still pretty large even after removing the invalid data points. The average time covered is 3 seconds which seems realistic if we consider, that we removed a large number of fixation points and some time is needed for the saccade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) visualizing data distribution [3 pts]\n",
    "Are there any inter-subject difference and effect of different category in our data?\n",
    "- plot the mean duration for each category (4 x-values) and the mean duration for each subject (48 x-values).\n",
    "\n",
    "- plot the mean number of fixations for each category (4 x-values) and the mean number of fixation for each subject (48 x-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axes = plt.subplots(2,2)\n",
    "fig.set_size_inches(10,9)\n",
    "# Make a barplot of the mean duration for each category\n",
    "axes[0,0].bar(clean_df.groupby('decoded category')['duration'].mean().index, clean_df.groupby('decoded category')['duration'].mean().values)\n",
    "# Set y limits to appropriate values\n",
    "axes[0,0].set(ylim=(200,280), title='Mean Duration per Category', ylabel='Time in Milliseconds')\n",
    "# Make a barplot of the mean number of fixations per trial per subject\n",
    "axes[0,1].bar(clean_df.groupby('SUBJECTINDEX')['duration'].mean().index,\n",
    "            clean_df.groupby('SUBJECTINDEX')['duration'].mean().values)\n",
    "# Set y limits to appropriate values\n",
    "axes[0,1].set(ylim=(200,280), title='Mean Duration per Subject')\n",
    "\n",
    "# Make a barplot of the mean number of fixations for each category\n",
    "axes[1,0].bar(clean_df.groupby(['decoded category','SUBJECTINDEX', 'trial']).count().groupby('decoded category').mean().index, clean_df.groupby(['category','SUBJECTINDEX', 'trial']).count().groupby('category').mean()['y'].values)\n",
    "axes[1,0].set(ylim=(6,15), title='Mean Number of Fixations per Trial per Category', xlabel='Categories', ylabel='Number of Fixations')\n",
    "\n",
    "# Make a barplot of the mean number of fixations per Subject\n",
    "axes[1,1].bar(clean_df.groupby(['category','SUBJECTINDEX', 'trial']).count().groupby('SUBJECTINDEX').mean().index,clean_df.groupby(['category','SUBJECTINDEX', 'trial']).count().groupby('SUBJECTINDEX').mean()['y'].values)\n",
    "axes[1,1].set(ylim=(6,20), title='Mean Number of Fixations per Trial per Subject', xlabel='SubjectID', )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) normally distributed data [3 pts]\n",
    "As ANOVA and lots of other statistical tests can be used only for normally distributed varaibles, it makes sense to find out whether the fixation duration and the number of fixations per trial is also normally distributed. This can be done with a Q-Q-Plot, which  is a graphical technique for determining if two data sets come from populations with a common distribution, in this case a normal distribution (for more information, klick [here](https://www.itl.nist.gov/div898/handbook/eda/section3/eda33o.htm)).\n",
    "- make a Q-Q plot of the variable *fixation duration*. Is it approximately normally distributed?\n",
    "- make a Q-Q plot of the variable *# fixation per trial*. Is it approximately normally distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "figure, ax = plt.subplots(2)\n",
    "# Plot the duration of the fixations against a normal distribution\n",
    "prob = sm.ProbPlot(clean_df['duration'], dist='norm')\n",
    "prob.qqplot(line='r', ax=ax[0])\n",
    "# Plot the  fixations per trial against a normal distribution\n",
    "prob = sm.ProbPlot(fixations_per_trial, dist='norm')\n",
    "prob.qqplot(line='r', ax=ax[1])\n",
    "ax[0].set_title('Q-Q plot of fixation duration versus normal distribution')\n",
    "ax[1].set_title('Q-Q plot of number of fixations per trial versus normal distribution')\n",
    "figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two variable seem not to be normally distributed. There seems to be a systematic difference between the normal and the the underlying distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Hypothesis testing [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) one way ANOVA [3 pts]\n",
    "Now it's time to really prove our intuition. Test the following null hypotheses:  \n",
    "$H01$: There is no difference in fixation duration across 4 different categories.  \n",
    "$H02$: There is no difference in the number of fixations across 4 different categories.\n",
    "\n",
    "What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('duration ~ category', clean_df).fit()\n",
    "\n",
    "#create anovas typ 1 and print them\n",
    "table = sm.stats.anova_lm(model, typ=1)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ols('fix ~ category', clean_df).fit()\n",
    "\n",
    "#create anovas typ 1 and print them\n",
    "table = sm.stats.anova_lm(model, typ=1)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what do you say?\n",
    "\n",
    "Based on the ANOVA tables both Hypotheses seems to be correct, as the result is in the significant 5% range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Binning [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) distribution of # fixation [2 pts]\n",
    "Now we would like to know if there's a difference in terms of fixation numbers at different time segment.\n",
    "- make a histogram with x axis being the start time of fixation\n",
    "- the bin size should be 1 sec. In total there would be 6 bins.\n",
    "- average over all subjects and images, just make a one simple plot\n",
    "- figure out mean fixation duration for each bin and print it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) leftward bias [2 pts]\n",
    "If you took Action & Cognition I, you have probably heard about the leftward bias in human's fixation behavior. It is known that people tend to look more at the left visual field for the initial first second. Check whether this holds for our dataset as well.\n",
    "- make a pointplot with x-axis as the x-coordinate of each fixation point and y-axis as the time bin to which the fixation point belongs to.\n",
    "- mark the confidence interval around each point.\n",
    "- make a vertical line at middle point of the x values. The x-coordinate of the fixation point in the data is based on the coordinate system using the display resolution. You can find out the information about display resolution by taking a look at the meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Heatmap [4 pts]\n",
    "Now let's use heatmap to visualize which part of pictures is fixated for how long. It would be also good to know if there's a difference between categories.\n",
    "- make a heatmap for data consisting of each categories and then averaged across all data.\n",
    "- mark a central point in the plot.\n",
    "- you can use ```numpy.histogram2d``` to compute a bi-dimensional histogram.\n",
    "- then you can use ```pyplot.imshow``` to plot these histogram.\n",
    "- don't forget to use the parameter ```extent``` to control for the bounding box to which the image should fit in.\n",
    "- if you want to use other functions that's surely fine as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots = 5\n",
    "fig, axes = plt.subplots(nrows=1, ncols=plots, sharey=True, figsize=(15, 4))\n",
    "\n",
    "# iterate through the categories and make a heatmap for each\n",
    "for counter, item in enumerate(set(clean_df['decoded category'])):\n",
    "    # heatmap for this category\n",
    "    axes[counter].scatter(clean_df[clean_df['decoded category'] == item]['x'], clean_df[clean_df['decoded category'] == item]['y'], c=clean_df[clean_df['decoded category'] == item]['duration'], s=100, cmap=plt.cm.get_cmap('hot'))\n",
    "    # set category as title\n",
    "    axes[counter].set_title(item)\n",
    "    # set the mean point\n",
    "    p1 = mpatches.FancyBboxPatch((clean_df[clean_df['decoded category'] == item]['x'].mean(),clean_df[clean_df['decoded category'] == item]['y'].mean()),width = 50, height = 25, color='#00BFFF')\n",
    "    axes[counter].add_patch(p1)\n",
    "# make heatmap for all categories together\n",
    "axes[4].scatter(clean_df['x'], clean_df['y'], c=clean_df['duration'], s=100, cmap=plt.cm.get_cmap('hot'))\n",
    "# set title for heatmap\n",
    "axes[4].set_title('All Categotries')\n",
    "#set the mean point\n",
    "p1 = mpatches.FancyBboxPatch((clean_df['x'].mean(),clean_df['y'].mean()),width = 50, height = 25, color='#00BFFF')\n",
    "axes[4].add_patch(p1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# make the bidimensional histogram\n",
    "H, xedges, yedges = np.histogram2d(clean_df['x'], clean_df['y'])\n",
    "plt.imshow(H, interpolation='gaussian', extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5: Scanpath [5 pts]\n",
    "We want to know whether different scan behaviour is used when viewing images of different categories. Scanpath is a path followed by the eyes when viewing a stimulus. Scanpaths are useful for analyzing cognitive intent, interest, and salience. It has an advantage to a heatmap because the information about temporal structure of viewing behaviour can be plotted.\n",
    "- make four plots, one for each category.\n",
    "- randomely choose one trial for which you will plot a scanpath.\n",
    "- mark fixation points based on the x-y coordinates.\n",
    "- plot the *saccade path* from one fixation point to another one.\n",
    "- make sure that the order of fixation can be read out from the plot. E.g. earlier fixations could have a light color whereas later fixations could have dark colors.\n",
    "- also include information about the duration of each fixation. One way to do it is the use the size of the fixation marker.\n",
    "- plot the background image superimposed with the scanpath. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=1, sharey=True, figsize=(30,120))\n",
    "\n",
    "i = 0\n",
    "for category in set(clean_df['category']):\n",
    "    \n",
    "    #filter for category:\n",
    "    df_cat = clean_df[clean_df['category'] == category]\n",
    "    \n",
    "    #select subject at random\n",
    "    subject = np.random.choice(df_cat['SUBJECTINDEX'])\n",
    "    df_sub = df_cat[df_cat['SUBJECTINDEX'] == subject]\n",
    "    \n",
    "    #select trial at random\n",
    "    trial = np.random.choice(df_sub['trial'])\n",
    "    df_trial = df_sub[df_sub['trial'] == trial]\n",
    "    \n",
    "    #plot the points of fixiations, color=starttime orderd from white to black, size=duration\n",
    "    axes[i].scatter(df_trial['x'], df_trial['y'], c=df_trial['start'], cmap=plt.cm.get_cmap('hot'), marker='o', s=df_trial['duration']*5)\n",
    "    \n",
    "    #connect the dots for better path saliency\n",
    "    axes[i].plot(df_trial['x'], df_trial['y'], '-.', color='red')\n",
    "   \n",
    "    #pull the image and use as background\n",
    "    axes[i].imshow(plt.imread('datas/{}/{}.png'.format(int(category),int(trial%64))), extent=[0, 1280, 0, 960])\n",
    "    \n",
    "    #set title\n",
    "    decoded_category = {7: 'Natural', 8: 'Urban', 10: 'Fractals', 11: 'Pink Noise'}\n",
    "    axes[i].set_title(decoded_category[category] + ', Subject: ' +  str(int(subject)) + ' Trial: ' + str(int(trial%64)), fontsize=20)\n",
    "    \n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation\n",
    "Wilming N, Onat S, OssandÃ³n J, Acik A, Kietzmann TC, Kaspar K, Gameiro RR, Vormberg A, KÃ¶nig P (2017) An extensive dataset of eye movements during viewing of complex images. Scientific Data 4: 160126. https://doi.org/10.1038/sdata.2016.126  \n",
    "Wilming N, Onat S, OssandÃ³n J, Acik A, Kietzmann TC, Kaspar K, Gameiro RR, Vormberg A, KÃ¶nig P (2017) Data from: An extensive dataset of eye movements during viewing of complex images. Dryad Digital Repository. https://doi.org/10.5061/dryad.9pf75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
